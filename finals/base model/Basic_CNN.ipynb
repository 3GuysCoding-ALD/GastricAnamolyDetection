{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic_CNN.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiTspnc4wldt"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3j5CVj-xAp-"
      },
      "source": [
        "cp /content/drive/MyDrive/base_model/kvasir-dataset-v2.zip ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UyGGl8NxA4z"
      },
      "source": [
        "! 7z x kvasir-dataset-v2.zip -aoa -o/content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJjc_OPTymzj"
      },
      "source": [
        "import os\r\n",
        "import numpy as np\r\n",
        "import seaborn as sea\r\n",
        "import tensorflow as tf \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "import tensorflow.keras.applications as Applications\r\n",
        "import tensorflow.keras.preprocessing.image as ImagePreprocessing\r\n",
        "import cv2\r\n",
        "import pickle\r\n",
        "import json\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.layers import Conv2D, Dense ,MaxPool2D ,BatchNormalization,Flatten ,Dropout\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\r\n",
        "from PIL import Image\r\n",
        "import importlib.util\r\n",
        "import random as rn\r\n",
        "import pandas as pd\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from shutil import copyfile\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\r\n",
        "import json\r\n",
        "import tensorflow.keras as keras\r\n",
        "\r\n",
        "sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IQsHxO6zCqb"
      },
      "source": [
        "train_split = 0.7\r\n",
        "val_split = 0.15\r\n",
        "test_split = 0.15\r\n",
        "original_ds_path = 'kvasir-dataset-v2'\r\n",
        "train_ds_path = 'train'\r\n",
        "val_ds_path = 'validate'\r\n",
        "test_ds_path = 'test'\r\n",
        "base_dir = '/content/drive/MyDrive/base_model/'\r\n",
        "losses_dir = base_dir+'losses/'\r\n",
        "model_save= base_dir+'model_saves/'\r\n",
        "seed = 49\r\n",
        "size_img = (224,224) #size choosen to preserve the aspect ratio\r\n",
        "input_size = (224,224,3)\r\n",
        "batch_size = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_iJb34HzMiz"
      },
      "source": [
        "ds_dic = {}\r\n",
        "for folder in os.listdir(original_ds_path):\r\n",
        "  ds_dic[folder] = os.listdir(original_ds_path+'/'+folder) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff_WzRuxzUHa"
      },
      "source": [
        "if not os.path.exists(train_ds_path):\r\n",
        "      os.mkdir(train_ds_path)\r\n",
        "if not os.path.exists(val_ds_path):\r\n",
        "      os.mkdir(val_ds_path)\r\n",
        "if not os.path.exists(test_ds_path):\r\n",
        "      os.mkdir(test_ds_path)\r\n",
        "\r\n",
        "for each in ds_dic:\r\n",
        "    \r\n",
        "    if not os.path.exists(train_ds_path+'/'+each+'/'):\r\n",
        "        os.mkdir(train_ds_path+'/'+each+'/')\r\n",
        "    if not os.path.exists(val_ds_path+'/'+each+'/'):\r\n",
        "        os.mkdir(val_ds_path+'/'+each+'/')\r\n",
        "    if not os.path.exists(test_ds_path+'/'+each+'/'):\r\n",
        "        os.mkdir(test_ds_path+'/'+each+'/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrfCJeaZzaSX"
      },
      "source": [
        "for each in ds_dic.keys():\r\n",
        "  train_split , intermediate_split=train_test_split(ds_dic[each], \r\n",
        "                                              test_size=0.3, \r\n",
        "                                              random_state=seed,\r\n",
        "                                              shuffle = True)\r\n",
        "  \r\n",
        "  val_split , test_split = train_test_split(intermediate_split, \r\n",
        "                                              test_size=0.5, \r\n",
        "                                              random_state=seed,\r\n",
        "                                              shuffle = True)\r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  for img in train_split:\r\n",
        "        destination = train_ds_path+'/'+each+'/'+img\r\n",
        "        source = original_ds_path+'/'+each+'/'+img\r\n",
        "        \r\n",
        "        if not os.path.exists(destination):\r\n",
        "            copyfile(source, destination)\r\n",
        "        \r\n",
        "  for img in test_split:\r\n",
        "      destination = test_ds_path+'/'+each+'/'+img\r\n",
        "      source = original_ds_path+'/'+each+'/'+img\r\n",
        "      \r\n",
        "      if not os.path.exists(destination):\r\n",
        "          copyfile(source, destination)\r\n",
        "\r\n",
        "  for img in val_split:\r\n",
        "      destination = val_ds_path+'/'+each+'/'+img\r\n",
        "      source = original_ds_path+'/'+each+'/'+img\r\n",
        "      \r\n",
        "      if not os.path.exists(destination):\r\n",
        "          copyfile(source, destination)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La05xSdZzkn5"
      },
      "source": [
        "train_datagen = ImageDataGenerator(featurewise_center=True,\r\n",
        "                               samplewise_center=True,\r\n",
        "                               featurewise_std_normalization=True,\r\n",
        "                               samplewise_std_normalization=True,\r\n",
        "                               zca_whitening=False,\r\n",
        "                               zca_epsilon=1e-06,\r\n",
        "                               rotation_range=180,\r\n",
        "                               width_shift_range=0.0,\r\n",
        "                               height_shift_range=0.0, \r\n",
        "                               brightness_range=None, \r\n",
        "                               shear_range=0.0, \r\n",
        "                               zoom_range=[0.0,2.0],\r\n",
        "                               channel_shift_range=0.0, \r\n",
        "                               fill_mode='nearest', \r\n",
        "                               cval=0.0, \r\n",
        "                               horizontal_flip=True,\r\n",
        "                               vertical_flip=True, \r\n",
        "                               rescale=1./255, \r\n",
        "                               preprocessing_function=None,\r\n",
        "                               validation_split=0.15\r\n",
        "    )\r\n",
        "\r\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\r\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjTAPqFXzq7T"
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(directory=train_ds_path,\r\n",
        "                                                    target_size=size_img,\r\n",
        "                                                    batch_size=batch_size,\r\n",
        "                                                    shuffle=True, \r\n",
        "                                                    seed=seed,\r\n",
        "                                                    )\r\n",
        "val_generator = val_datagen.flow_from_directory(directory=val_ds_path,\r\n",
        "                                                    target_size=size_img,\r\n",
        "                                                    batch_size=batch_size,\r\n",
        "                                                    shuffle=True, \r\n",
        "                                                    seed=seed,\r\n",
        "                                                    )\r\n",
        "test_generator = test_datagen.flow_from_directory(directory=test_ds_path,\r\n",
        "                                                    target_size=size_img,\r\n",
        "                                                    batch_size=batch_size,\r\n",
        "                                                    shuffle=True, \r\n",
        "                                                    seed=seed,\r\n",
        "                                                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQxNQ0xCz3qK"
      },
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1NRKrXw0Yme"
      },
      "source": [
        "model = keras.Sequential()\r\n",
        "model.add(Conv2D(512,(3,3),activation='relu',input_shape = input_size))\r\n",
        "model.add(MaxPool2D(2,2))\r\n",
        "model.add(Conv2D(512,(3,3),activation='relu'))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(MaxPool2D(2,2))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Conv2D(512,(3,3),activation='relu'))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(MaxPool2D(2,2))\r\n",
        "model.add(Conv2D(512,(3,3),activation='relu'))\r\n",
        "model.add(MaxPool2D(2,2))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(128,activation='relu'))\r\n",
        "model.add(Dense(8,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzW40jY_0g9w"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyW_JYGE1R5L"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adamax(lr=1e-2), \r\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\r\n",
        "              metrics=[tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\r\n",
        "                       tf.keras.metrics.CategoricalCrossentropy(name='crossentropy_loss'),\r\n",
        "                       ]\r\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6rSTinwD2tf"
      },
      "source": [
        "%ls /content/drive/MyDrive/base_model/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uvG2pJe-EKQ"
      },
      "source": [
        "import os\r\n",
        "try:\r\n",
        "  if (not os.path.exists(base_dir+'model_saves/')):\r\n",
        "    os.mkdir(base_dir+'model_saves/')\r\n",
        "except FileExistsError:\r\n",
        "  print('File Exists')\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8x-niIyMCUC"
      },
      "source": [
        "train_history_list = {'initial':[],'secondary':[]}\r\n",
        "val_history_list = {'initial':[],'secondary':[]}\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmHgqAvm1v4L"
      },
      "source": [
        "\r\n",
        "history = model.fit_generator(\r\n",
        "    generator=train_generator, steps_per_epoch=None, epochs=50, verbose=1, callbacks=None,\r\n",
        "    validation_data=val_generator,\r\n",
        "    shuffle=True, initial_epoch=0)\r\n",
        "\r\n",
        "train_history_list['initial'].append(history)\r\n",
        "\r\n",
        "model.save_weights(model_save+str(times)+'_init.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}